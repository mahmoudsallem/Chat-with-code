{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with your code! </>\n",
    "\n",
    "\n",
    "<img src=\"chat_with_code.png\" width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/teamspace/studios/this_studio/weights\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/teamspace/studios/this_studio/weights\"\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import uuid\n",
    "import textwrap\n",
    "import subprocess\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "from rag_101.retriever import (\n",
    "    load_embedding_model,\n",
    "    load_reranker_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows nested access to the event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the llm\n",
    "llm=Ollama(model=\"mistral\", request_timeout=60.0)\n",
    "\n",
    "# setting up the embedding model\n",
    "lc_embedding_model = load_embedding_model()\n",
    "embed_model = LangchainEmbedding(lc_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def parse_github_url(url):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "def clone_github_repo(repo_url):    \n",
    "    try:\n",
    "        print('Cloning the repo ...')\n",
    "        result = subprocess.run([\"git\", \"clone\", repo_url], check=True, text=True, capture_output=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to clone repository: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a query engine\n",
    "\n",
    "def setup_query_engine(github_url):\n",
    "    \n",
    "    owner, repo = parse_github_url(github_url)\n",
    "    \n",
    "    if validate_owner_repo(owner, repo):\n",
    "        # Clone the GitHub repo & save it in a directory\n",
    "        input_dir_path = f\"/teamspace/studios/this_studio/{repo}\"\n",
    "\n",
    "        if os.path.exists(input_dir_path):\n",
    "            pass\n",
    "        else:\n",
    "            clone_github_repo(github_url)\n",
    "        \n",
    "        loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".py\", \".ipynb\", \".js\", \".ts\", \".md\"],\n",
    "            recursive=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            docs = loader.load_data()\n",
    "\n",
    "            # ====== Create vector store and upload data ======\n",
    "            Settings.embed_model = embed_model\n",
    "            index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
    "            # TODO try async index creation for faster emebdding generation & persist it to memory!\n",
    "            # index = VectorStoreIndex(docs, use_async=True)\n",
    "\n",
    "            # ====== Setup a query engine ======\n",
    "            Settings.llm = llm\n",
    "            query_engine = index.as_query_engine(similarity_top_k=4)\n",
    "            \n",
    "            # ====== Customise prompt template ======\n",
    "            qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "            qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "            query_engine.update_prompts(\n",
    "                {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    "            )\n",
    "\n",
    "            if docs:\n",
    "                print(\"Data loaded successfully!!\")\n",
    "                print(\"Ready to chat!!\")\n",
    "            else:\n",
    "                print(\"No data found, check if the repository is not empty!\")\n",
    "            \n",
    "            return query_engine\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        print('Invalid github repo, try again!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning the repo ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c147b130c5e8472aaede4b54f4a13a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1d6ade9e164ea2ba1decb9e63a1d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!!\n",
      "Ready to chat!!\n"
     ]
    }
   ],
   "source": [
    "# Provide url to the repository you want to chat with\n",
    "github_url = \"https://github.com/Lightning-AI/lit-gpt\"\n",
    "\n",
    "query_engine = setup_query_engine(github_url=github_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " To finetune a Large Language Model (LLM) using Lit-GPT, follow these steps:\n",
       "\n",
       "1. Install the required packages: First, install EleutherAI's lm-eval framework by running the following command in your terminal or command prompt:\n",
       "   ```bash\n",
       "   pip install https://github.com/EleutherAI/lm-evaluation-harness/archive/refs/heads/master.zip -U\n",
       "   ```\n",
       "2. Familiarize yourself with the conceptual tutorials: To gain a better understanding of finetuning, read the articles available in the `README.md` file under the `tutorials` directory. These include \"Understanding Parameter-Efficient Finetuning of Large Language Models\" and \"Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)\".\n",
       "3. Choose a finetuning method: Lit-GPT supports various finetuning methods, including using adapters and LoRA or QLoRA. Familiarize yourself with these methods by reading the corresponding how-to guides available in the `README.md` file under the `tutorials/finetune` directory.\n",
       "4. Prepare your dataset: Make sure you have a suitable dataset for finetuning the LLM. The data should be formatted correctly and easily loadable by Lit-GPT.\n",
       "5. Finetune with Adapters (Optional): If you choose to use adapters for finetuning, follow the instructions provided in the \"Finetune with Adapters\" guide in the `README.md` file under the `tutorials/finetune` directory.\n",
       "6. Finetune with LoRA or QLoRA (Optional): If you choose to use LoRA or QLoRA for finetuning, follow the instructions provided in the \"Finetune with LoRA or QLoRA\" guide in the `README.md` file under the `tutorials/finetune` directory.\n",
       "7. Evaluate your finetuned model: Once you have finished finetuning your LLM, evaluate its performance using EleutherAI's lm-eval framework. Use the following command to evaluate the model on all tasks in Eleuther AI's Evaluation Harness:\n",
       "   ```bash\n",
       "   python eval/lm_eval_harness.py \\\n",
       "       --checkpoint_dir \"[path_to_your_finetuned_model_directory]\" \\\n",
       "       --precision \"bf16-true\" \\\n",
       "       --save_filepath \"results.json\"\n",
       "   ```\n",
       "   To evaluate the model on specific tasks, use the `--eval_tasks` flag:\n",
       "   ```bash\n",
       "   python eval/lm_eval_harness.py \\\n",
       "       --checkpoint_dir \"[path_to_your_finetuned_model_directory]\" \\\n",
       "       --eval_tasks \"[task1, task2]\" \\\n",
       "       --precision \"bf16-true\" \\\n",
       "       --save_filepath \"results.json\"\n",
       "   ```\n",
       "Replace `[path_to_your_finetuned_model_directory]` with the path to your finetuned model directory, and replace `task1` and `task2` with the names of the evaluation tasks you want to use.\n",
       "8. (Optional) Analyze the results: Once you have evaluated your model, analyze the results provided in the output file to assess its performance on various tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query('Can you provide a step by step guide to finetuning an llm using lit-gpt')\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
